import requests
import pandas as pd
import numpy  as np

import random
from scipy import sparse
from scipy.sparse.linalg import spsolve
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MaxAbsScaler

from travels.models import UserLogData


def init():
    userLogDatas = UserLogData.objects.all()    
    userlog_df = pd.DataFrame(
      [
        [userLogData.user.username, userLogData.travelspot.content_id, userLogData.travelspot.title, userLogData.click, userLogData.date]
        for userLogData in userLogDatas
      ],
      columns = ['user', 'content_id', 'content_title', 'click', 'date']
    )
    return userlog_df

def make_train (matrix, percentage = .2):
    '''
    -----------------------------------------------------
    ì„¤ëª…
    ìœ ì €-ì•„ì´í…œ í–‰ë ¬ (matrix)ì—ì„œ 
    1. 0ì´ìƒì˜ ê°’ì„ ê°€ì§€ë©´ 1ì˜ ê°’ì„ ê°–ë„ë¡ binaryí•˜ê²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“¤ê³ 
    2. í›ˆë ¨ ë°ì´í„°ëŠ” ì›ë³¸ í–‰ë ¬ì—ì„œ percentage ë¹„ìœ¨ë§Œí¼ 0ìœ¼ë¡œ ë°”ë€œ
    
    -----------------------------------------------------
    ë°˜í™˜
    training_set: í›ˆë ¨ ë°ì´í„°ì—ì„œ percentage ë¹„ìœ¨ë§Œí¼ 0ìœ¼ë¡œ ë°”ë€ í–‰ë ¬
    test_set:     ì›ë³¸ ìœ ì €-ì•„ì´í…œ í–‰ë ¬ì˜ ë³µì‚¬ë³¸
    user_inds:    í›ˆë ¨ ë°ì´í„°ì—ì„œ 0ìœ¼ë¡œ ë°”ë€ ìœ ì €ì˜ index
    '''
    test_set = matrix.copy()
    test_set[test_set !=0] = 1 # binaryí•˜ê²Œ ë§Œë“¤ê¸°
    
    training_set = matrix.copy()
    nonzero_inds = training_set.nonzero()
    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1]))
    
    random.seed(0)
    num_samples = int(np.ceil(percentage * len(nonzero_pairs)))
    samples = random.sample(nonzero_pairs, num_samples)
    
    user_inds = [index[0] for index in samples]
    item_inds = [index[1] for index in samples]
    
    # eliminate_zeros(): Remove zero entries from the matrix
    training_set[user_inds, item_inds] = 0
    training_set.eliminate_zeros()
    
    return training_set, test_set, list(set(user_inds))


def main(username):
    userlog_df = init()
    item_lookup = userlog_df[['content_id','content_title']].drop_duplicates()
    item_lookup['content_id'] = item_lookup['content_id']
    cleaned_log = userlog_df[['user','content_id','click']]
    grouped_cleaned = cleaned_log.groupby(['user','content_id']).sum().reset_index() #reset_index() í–‰ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    grouped_purchased = grouped_cleaned[grouped_cleaned['click'] > 0]
    customers = list(np.sort(grouped_purchased['user'].unique()))
    products = list (grouped_purchased['content_id'].unique())
    quantity = list(grouped_purchased['click'])
    rows = grouped_purchased['user'].astype('category').cat.codes 
    cols = grouped_purchased['content_id'].astype('category').cat.codes
    purchase_sparse = sparse.csr_matrix((quantity, (rows, cols)), shape = (len(customers),len(products)))

    product_train, product_test, product_users_altered = make_train(purchase_sparse, 0.2)

    
    pref_vec = product_train[0,:].toarray() # Get the ratings from the training set ratings matrix
    pref_vec = pref_vec.reshape(-1) + 1 # Add 1 to everything, so that items not purchased yet become equal to 1
    pref_vec[pref_vec > 1] = 0 # Make everything already purchased zero

    user_vecs, item_vecs = implicit_weighted_ALS (product_train
                                              , lambda_val = 0.1
                                              , alpha = 15
                                              , n_iter = 10
                                              , rank_size = 20)
    
    rec_vector = user_vecs[0,:].dot(item_vecs) # Get dot product of user vector and all item vectors
    rec_vector = rec_vector.toarray()

    # Scale this recommendation vector between 0 and 1
    min_max = MinMaxScaler()
    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0] 

    recommend_vector = pref_vec*rec_vector_scaled
    #print(recommend_vector)
    
    customers_arr = np.array(customers)
    products_arr = np.array(products)
    product_idx = np.argsort(recommend_vector)[::-1][:5]
    purchaseData = get_items_purchased(username, product_train, customers_arr, products_arr, item_lookup)
    recData = rec_items(username, product_train, user_vecs, item_vecs, customers_arr, products_arr, item_lookup, num_items = 10)
    return purchaseData.values.tolist(), recData.values.tolist()


def implicit_weighted_ALS(training_set, lambda_val =.1, alpha = 40, n_iter = 10, rank_size = 20, seed = 0):
    '''
    í˜‘ì—… í•„í„°ë§ì— ê¸°ë°˜í•œ ALS
    -----------------------------------------------------
    input
    1. training_set : m x n í–‰ë ¬ë¡œ, mì€ ìœ ì € ìˆ˜, nì€ ì•„ì´í…œ ìˆ˜ë¥¼ ì˜ë¯¸. csr í–‰ë ¬ (í¬ì†Œ í–‰ë ¬) í˜•íƒœì—¬ì•¼ í•¨ 
    2. lambda_val: ALSì˜ ì •ê·œí™” term. ì´ ê°’ì„ ëŠ˜ë¦¬ë©´ biasëŠ” ëŠ˜ì§€ë§Œ ë¶„ì‚°ì€ ê°ì†Œ. defaultê°’ì€ 0.1
    3. alpha: ì‹ ë¢° í–‰ë ¬ê³¼ ê´€ë ¨í•œ ëª¨ìˆ˜ (C_{ui} = 1 + alpha * r_{ui}). ì´ë¥¼ ê°ì†Œì‹œí‚¤ë©´ í‰ì  ê°„ì˜ ì‹ ë¢°ë„ì˜ ë‹¤ì–‘ì„±ì´ ê°ì†Œ
    4. n_iter: ë°˜ë³µ íšŸìˆ˜. ë…¼ë¬¸ì—ì„œëŠ” 10~ 15íšŒ ì •ë„ ì„¤ì •
    5. rank_size: ìœ ì €/ ì•„ì´í…œ íŠ¹ì„± ë²¡í„°ì˜ ìž ìž¬ íŠ¹ì„±ì˜ ê°œìˆ˜. ë…¼ë¬¸ì—ì„œëŠ” 20 ~ 200 ì‚¬ì´ë¥¼ ì¶”ì²œí•˜ê³  ìžˆìŒ. ì´ë¥¼ ëŠ˜ë¦¬ë©´ ê³¼ì í•© ìœ„í—˜ì„±ì´ ìžˆìœ¼ë‚˜ 
    biasê°€ ê°ì†Œ
    6. seed: ë‚œìˆ˜ ìƒì„±ì— í•„ìš”í•œ seed
    -----------------------------------------------------
    ë°˜í™˜
    ìœ ì €ì™€ ì•„ì´í…œì— ëŒ€í•œ íŠ¹ì„± ë²¡í„°
    '''
    
    # 1. Confidence matrix
    # C = 1+ alpha * r_{ui}
    conf = (alpha*training_set) # sparse í–‰ë ¬ í˜•íƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ 1ì„ ë‚˜ì¤‘ì— ë”í•¨
    
    num_user = conf.shape[0]
    num_item = conf.shape[1]

    # Xì™€ Y ì´ˆê¸°í™”
    rstate = np.random.RandomState(seed)
    X = sparse.csr_matrix(rstate.normal(size = (num_user, rank_size)))
    Y = sparse.csr_matrix(rstate.normal(size = (num_item, rank_size)))
    # sparse.eye: Sparse matrix with ones on diagonal 
    X_eye = sparse.eye(num_user)
    Y_eye = sparse.eye(num_item)
    
    # ì •ê·œí™” term: ð€I
    lambda_eye = lambda_val * sparse.eye(rank_size)
    
    # ë°˜ë³µ ì‹œìž‘
    for i in range(n_iter):
        yTy = Y.T.dot(Y)
        xTx = X.T.dot(X)
        
        # Yë¥¼ ê³ ì •í•´ë†“ê³  Xì— ëŒ€í•´ ë°˜ë³µ
        # Xu = (yTy + yT(Cu-I)Y + ð€I)^{-1} yTCuPu
        for u in range(num_user):
            conf_samp = conf[u,:].toarray() # Cu
            pref = conf_samp.copy()
            pref[pref!=0] = 1
            # Cu-I: ìœ„ì—ì„œ confì— 1ì„ ë”í•˜ì§€ ì•Šì•˜ìœ¼ë‹ˆê¹Œ Ië¥¼ ë¹¼ì§€ ì•ŠìŒ 
            CuI = sparse.diags(conf_samp, [0])
            # yT(Cu-I)Y
            yTCuIY = Y.T.dot(CuI).dot(Y)
            # yTCuPu
            yTCupu = Y.T.dot(CuI+Y_eye).dot(pref.T)
            
            X[u] = spsolve(yTy + yTCuIY + lambda_eye, yTCupu)
        
        # Xë¥¼ ê³ ì •í•´ë†“ê³  Yì— ëŒ€í•´ ë°˜ë³µ
        # Yi = (xTx + xT(Cu-I)X + ð€I)^{-1} xTCiPi
        for i in range(num_item):
            conf_samp = conf[:,i].T.toarray()
            pref = conf_samp.copy()
            pref[pref!=0] = 1
            
            #Ci-I
            CiI = sparse.diags(conf_samp, [0])
            # xT(Ci-I)X
            xTCiIX = X.T.dot(CiI).dot(X)
            # xTCiPi
            xTCiPi = X.T.dot(CiI+ X_eye).dot(pref.T)
            
            Y[i] = spsolve(xTx + xTCiIX + lambda_eye, xTCiPi)
            
        return X, Y.T

def get_items_purchased(customer_id, mf_train, customer_list, products_list, item_lookup):
    '''
    íŠ¹ì • ìœ ì €ê°€ êµ¬ë§¤í•œ ëª©ë¡ì„ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
    ----------------------------------------
    INPUT
    1. customer_id: ê³ ê° ID
    2. mf_train: í›ˆë ¨ ë°ì´í„° í‰ì 
    3. customers_list: í›ˆë ¨ ë°ì´í„°ì— ì“°ì¸ ê³ ê° ëª©ë¡
    4. products_list: í›ˆë ¨ ë°ì´í„°ì— ì“°ì¸ ì•„ì´í…œ ëª©ë¡
    5. item_lookup: ìœ ë‹ˆí¬í•œ ì•„ì´í…œ IDì™€ ì„¤ëª…ì„ ë‹´ì€ í…Œì´ë¸”
    '''
    cust_ind = np.where (customer_list == customer_id)[0][0]
    purchased_ind = mf_train[cust_ind,:].nonzero()[1]
    prod_codes = products_list[purchased_ind]
    
    return item_lookup[item_lookup.content_id.isin(prod_codes.tolist())]

def rec_items(customer_id, mf_train, user_vecs, item_vecs, customer_list, item_list, item_lookup, num_items = 10):
    '''
    ìœ ì €ì˜ ì¶”ì²œ ì•„ì´í…œ ë°˜í™˜
    -----------------------------------------------------
    INPUT
    1. customer_id - Input the customer's id number that you want to get recommendations for
    2. mf_train: í›ˆë ¨ ë°ì´í„°
    3. user_vecs: í–‰ë ¬ ë¶„í•´ì— ì“°ì¸ ìœ ì € ë²¡í„°
    4. item_vecs: í–‰ë ¬ ë¶„í•´ì— ì“°ì¸ ì•„ì´í…œ ë²¡í„°
    5. customer_list: í‰ì  í–‰ë ¬ì˜ í–‰ì— í•´ë‹¹í•˜ëŠ” ê³ ê° ID
    6. item_list: í‰ì  í–‰ë ¬ì˜ ì—´ì— í•´ë‹¹í•˜ëŠ” ì•„ì´í…œ ID
    7. item_lookup: ì•„ì´í…œ IDì™€ ì„¤ëª…ì„ ë‹´ì€ í…Œì´ë¸”
    8. num_items: ì¶”ì²œí•  ì•„ì´í…œ ê°œìˆ˜
    -----------------------------------------------------
    ë°˜í™˜    
    êµ¬ë§¤í•œ ì ì´ ì—†ëŠ” ì•„ì´í…œ ì¤‘ ì˜ˆì¸¡ í‰ì ì´ ë†’ì€ ìµœê³  nê°œì˜ ì¶”ì²œ ì•„ì´í…œ
    '''
    
    pref_vec = mf_train[0,:].toarray() # Get the ratings from the training set ratings matrix
    pref_vec = pref_vec.reshape(-1) + 1 # Add 1 to everything, so that items not purchased yet become equal to 1
    pref_vec[pref_vec > 1] = 0 # Make everything already purchased zero
    rec_vector = user_vecs[0,:].dot(item_vecs) # Get dot product of user vector and all item vectors
    rec_vector = rec_vector.toarray()

    # Scale this recommendation vector between 0 and 1
    min_max = MinMaxScaler()
    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0] 

    recommend_vector = pref_vec*rec_vector_scaled
    
    product_idx = np.argsort(recommend_vector)[::-1][:num_items] # Sort the indices of the items into order 
    # of best recommendations
    rec_list = [] # start empty list to store items
    
    for index in product_idx:
        code = item_list[index]
        rec_list.append([code, item_lookup.content_title.loc[item_lookup.content_id == code].iloc[0]]) 
        # Append our descriptions to the list
        
    codes = [item[0] for item in rec_list]
    descriptions = [item[1] for item in rec_list]
    final_frame = pd.DataFrame({'content_id': codes, 'content_title': descriptions}) # Create a dataframe 
    return final_frame[['content_id', 'content_title']] # Switch order of columns around

